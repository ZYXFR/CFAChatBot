{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to reset the state\n",
    "def reset_state():\n",
    "    for key in st.session_state:\n",
    "        del st.session_state[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE: str = \"cuda\"\n",
    "# MODEL_NAME: str = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "MODEL_NAME: str = \"/disk2/elvys/Mistral-7B-Instruct-v0.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM:\n",
    "    def __init__(self) -> None:\n",
    "        # self.model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16).to(DEVICE)\n",
    "        self.model.eval()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    def __call__(self, messages: list) -> str:\n",
    "        # Tokenize messages\n",
    "        model_inputs = self.tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(DEVICE)\n",
    "        # Generate answer for the given input\n",
    "        with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n",
    "            generated_ids = self.model.generate(model_inputs, max_new_tokens=1100, do_sample=False)\n",
    "            decoded = self.tokenizer.batch_decode(generated_ids)\n",
    "        return decoded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.set_page_config(\n",
    "    layout=\"wide\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@st.cache_resource\n",
    "def load_llm_model() -> LLM:\n",
    "    return LLM()\n",
    "llm_model = load_llm_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the initial value of widgets in session state\n",
    "if \"disabled\" not in st.session_state:\n",
    "    st.session_state.disabled = False\n",
    "    st.session_state.messages = []\n",
    "\n",
    "with st.sidebar:\n",
    "    st.title('Rene: Investment assistant')\n",
    "    assistant_type = st.selectbox('Select assistant type:', [\"AI Technical analysis\", \"Chatbot\"], index=0, disabled=st.session_state.disabled)\n",
    "    analysis_type = st.selectbox('AI Analysis Style:', [\"Analytical\", \"Advisory\"], index=0, disabled=st.session_state.disabled)\n",
    "    experience_user = st.selectbox('Knowledge Level:', [\"Novice\", \"Specialist\"], index=0, disabled=st.session_state.disabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if assistant_type == \"AI Technical analysis\":\n",
    "    with st.sidebar:\n",
    "        button = st.button('Generate AI Technical analysis', disabled=st.session_state.disabled)\n",
    "    if button and assistant_type == \"AI Technical analysis\":\n",
    "        st.session_state.disabled = True\n",
    "        if selected_asset == \"General\":\n",
    "            st.markdown(utils.convert_str_to_markdown(\"Select an asset to get the AI Technical analysis.\"))\n",
    "        else:\n",
    "            start = time.time()\n",
    "            prompt = prompts.get_prompt(\n",
    "                selected_asset, \n",
    "                experience_user,\n",
    "                analysis_type,\n",
    "                f\"Provide the financial analysis of {selected_asset}\", \n",
    "                prices[selected_asset], \n",
    "                smas[selected_asset], \n",
    "                support_resistances[selected_asset], \n",
    "                oscillators_values[selected_asset],\n",
    "                active_patterns[selected_asset], \n",
    "                anticipated_patterns[selected_asset],\n",
    "            )\n",
    "            print(\"PROMPT:\", prompt)\n",
    "            chat_model_response = llm_model([{\"role\": \"user\", \"content\": prompt}])\n",
    "            chat_model_response = chat_model_response.split(\"[/INST]\")[-1].split(\"</s>\")[0]\n",
    "            st.markdown(utils.convert_str_to_markdown(chat_model_response))\n",
    "            print(\"Inference duration:\", time.time() - start)\n",
    "        st.session_state.disabled = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit run streamlit101.ipynb\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
